{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"localhost\"\n",
    "PORT = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 19:56:07 WARN Utils: Your hostname, daniel-Yoga-Creator-7-15IMH05 resolves to a loopback address: 127.0.1.1; using 192.168.63.125 instead (on interface wlp0s20f3)\n",
      "24/09/30 19:56:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/30 19:56:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"StructuredNetworkCount\") \\\n",
    "    .config(\"spark.driver.host\",\"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 19:56:10 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "data = (\n",
    "    spark.readStream.format(\"socket\")\n",
    "    .option(\"host\", HOST)\n",
    "    .option(\"port\", PORT)\n",
    "    # .option(\"includeTimestamp\", \"true\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "    .selectExpr(\"from_json(json, 'timestamp STRING, value INT') as data\") \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "data = data.withColumn(\"dummy_key\", lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.streaming.state import GroupStateTimeout\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
    "\n",
    "\n",
    "outputSchema = StructType([StructField(\"mean\", DoubleType(), True), StructField(\"std\", DoubleType(), True)])\n",
    "stateSchema = StructType([StructField(\"S\", IntegerType(), True), StructField(\"n\", IntegerType(), True), StructField(\"Q\", DoubleType(), True)])\n",
    "\n",
    "def update_count(key, df_iter, state):\n",
    "    if not state.exists:\n",
    "        state.update((0, 0, 0))\n",
    "\n",
    "    S, n, Q = state.get\n",
    "\n",
    "    for df in df_iter:\n",
    "        for row in df.itertuples(index=False):\n",
    "            value = int(row.value)\n",
    "            n += 1\n",
    "            S += value\n",
    "            Q += Q + (value - S/n) ** 2\n",
    "    \n",
    "    state.update((S, n, Q))\n",
    "\n",
    "    mean = S/n\n",
    "    std = (1/n * Q) ** 1/2\n",
    "\n",
    "    yield pd.DataFrame([(mean, std)], columns=outputSchema.fieldNames())\n",
    "result = data.groupBy(\"dummy_key\").applyInPandasWithState(\n",
    "    update_count,\n",
    "    outputStructType=outputSchema,\n",
    "    stateStructType=stateSchema,\n",
    "    outputMode=\"append\",\n",
    "    timeoutConf=GroupStateTimeout.NoTimeout\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 19:56:11 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-8362e930-2449-437f-ba4b-360381fb9f60. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/09/30 19:56:11 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    result.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"console\")\n",
    "    .trigger(processingTime=\"2 seconds\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+--------+\n",
      "|mean|variance|\n",
      "+----+--------+\n",
      "+----+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 19:56:20 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 8873 milliseconds\n",
      "/home/daniel/miniconda3/envs/sparky/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:922: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "24/09/30 19:56:25 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 5238 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------------+--------------------+\n",
      "|             mean|            variance|\n",
      "+-----------------+--------------------+\n",
      "|51.84705882352941|6.395589245296199E25|\n",
      "+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/sparky/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:922: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "24/09/30 19:56:31 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 5552 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----------------+--------------------+\n",
      "|             mean|            variance|\n",
      "+-----------------+--------------------+\n",
      "|49.62043795620438|1.787058200046796E41|\n",
      "+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/sparky/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:922: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "24/09/30 19:56:44 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 13438 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+------------------+-------------------+\n",
      "|              mean|           variance|\n",
      "+------------------+-------------------+\n",
      "|49.557291666666664|4.59417777584733E57|\n",
      "+------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/sparky/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:922: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+------------------+--------------------+\n",
      "|              mean|            variance|\n",
      "+------------------+--------------------+\n",
      "|50.246153846153845|2.955391960843945...|\n",
      "+------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 19:56:58 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 13713 milliseconds\n",
      "/home/daniel/miniconda3/envs/sparky/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:922: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "24/09/30 19:57:07 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 8623 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-----------------+--------------------+\n",
      "|             mean|            variance|\n",
      "+-----------------+--------------------+\n",
      "|50.44347826086957|9.094734630258217...|\n",
      "+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/sparky/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:922: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "24/09/30 19:57:11 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 4108 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+-----------------+--------------------+\n",
      "|             mean|            variance|\n",
      "+-----------------+--------------------+\n",
      "|50.25504587155963|2.969620772314853...|\n",
      "+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/sparky/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:922: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "24/09/30 19:57:15 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 3883 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+-----------------+--------------------+\n",
      "|             mean|            variance|\n",
      "+-----------------+--------------------+\n",
      "|50.43344709897611|6.073369454739601...|\n",
      "+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/sparky/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:922: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "24/09/30 19:57:18 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 3794 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+-----------------+--------------------+\n",
      "|             mean|            variance|\n",
      "+-----------------+--------------------+\n",
      "|50.39903846153846|1.567770767814801...|\n",
      "+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/sparky/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:922: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "24/09/30 19:57:22 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 4012 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+-----------------+--------------------+\n",
      "|             mean|            variance|\n",
      "+-----------------+--------------------+\n",
      "|50.37613293051359|4.062084916440338...|\n",
      "+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/sparky/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:922: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "24/09/30 19:57:26 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 3848 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+------------------+--------------------+\n",
      "|              mean|            variance|\n",
      "+------------------+--------------------+\n",
      "|50.205128205128204|4.211819023218590...|\n",
      "+------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/sparky/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:922: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "24/09/30 19:57:30 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 3836 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+-----------------+--------------------+\n",
      "|             mean|            variance|\n",
      "+-----------------+--------------------+\n",
      "|50.00810810810811|1.098284689548016...|\n",
      "+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/sparky/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:922: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "24/09/30 19:57:34 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 3872 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+-----------------+--------------------+\n",
      "|             mean|            variance|\n",
      "+-----------------+--------------------+\n",
      "|49.75064267352185|2.871487217889251...|\n",
      "+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/sparky/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:922: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "24/09/30 19:57:38 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 3746 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "+-----------------+--------------------+\n",
      "|             mean|            variance|\n",
      "+-----------------+--------------------+\n",
      "|50.04651162790697|1.503260544181006...|\n",
      "+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/sparky/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:922: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "24/09/30 19:57:42 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 3846 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 14\n",
      "-------------------------------------------\n",
      "+-----------------+--------------------+\n",
      "|             mean|            variance|\n",
      "+-----------------+--------------------+\n",
      "|49.81967213114754|1.976552180821840...|\n",
      "+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/sparky/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:922: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "24/09/30 19:57:45 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 3849 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 15\n",
      "-------------------------------------------\n",
      "+-----------------+--------------------+\n",
      "|             mean|            variance|\n",
      "+-----------------+--------------------+\n",
      "|50.05829596412556|5.201650107299774...|\n",
      "+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/sparky/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:922: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "24/09/30 19:57:49 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 3903 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 16\n",
      "-------------------------------------------\n",
      "+-----------------+--------------------+\n",
      "|             mean|            variance|\n",
      "+-----------------+--------------------+\n",
      "|49.93333333333333|1.371395994818751...|\n",
      "+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/sparky/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:922: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "24/09/30 19:57:53 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 3852 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 17\n",
      "-------------------------------------------\n",
      "+----------------+--------------------+\n",
      "|            mean|            variance|\n",
      "+----------------+--------------------+\n",
      "|50.0092879256966|7.235888718304675...|\n",
      "+----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/sparky/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:922: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "ERROR:root:KeyboardInterrupt while sending command.            (108 + 12) / 200]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daniel/miniconda3/envs/sparky/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/daniel/miniconda3/envs/sparky/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/daniel/miniconda3/envs/sparky/lib/python3.12/socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[Stage 37:===================================================>  (192 + 8) / 200]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m query\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m~/miniconda3/envs/sparky/lib/python3.12/site-packages/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m~/miniconda3/envs/sparky/lib/python3.12/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/miniconda3/envs/sparky/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/miniconda3/envs/sparky/lib/python3.12/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sparky/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 18\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparky",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
